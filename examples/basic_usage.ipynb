{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of basic PIQARD library usage\n",
    "To use all of available components first you have to set environmental variables:\n",
    "* **COHERE_API_KEY** - for Cohere xlarge language model\n",
    "* **HUGGINGFACE_API_KEY** - for bloom and gpt j6b language models\n",
    "* **GOOGLE_CUSTOM_SEARCH_API_KEY** and **GOOGLE_CUSTOM_SEARCH_ENGINE_ID** - for google custom search information retriever\n",
    "\n",
    "Detailed description how to get access to needed keys can be found in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# set environment variables\n",
    "import os\n",
    "os.environ[\"COHERE_API_KEY\"] = \"eYDbpsxzTril5NSJTGhv3olRwtNuuAkl9WHK5Vl5\"\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
    "os.environ[\"GOOGLE_CUSTOM_SEARCH_API_KEY\"] = \"AIzaSyAB46rrYmTj6_w-7qCME3Gve7vqcUGzwAY\"\n",
    "os.environ[\"GOOGLE_CUSTOM_SEARCH_ENGINE_ID\"] = \"21b53499491814de3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from piqard.utils.prompt_template import PromptTemplate\n",
    "from piqard.utils.data_loaders import DatabaseLoaderFactory\n",
    "from piqard.information_retrievers import BM25Retriever, AnnoyRetriever, FAISSRetriever, GoogleCustomSearch, WikiAPI\n",
    "from piqard.language_models import CohereAPI, BLOOM176bAPI, GPTj6bAPI\n",
    "from piqard.PIQARD import PIQARD\n",
    "from piqard.extensions.react import Agent, Action\n",
    "from piqard.extensions.self_aware import SelfAware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language models:\n",
    "* CohereAPI\n",
    "* BLOOM176bAPI\n",
    "* GPTJ6bAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_query = \"How long do african elephants live?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The average lifespan of an African elephant is 60 years. The oldest recorded African elephant lived to'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloom = BLOOM176bAPI()\n",
    "bloom.query(example_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'African elephants live for about 70 years.\\nWhat is the largest land animal?\\nThe largest land animal is the African elephant.\\nWhat is the largest animal in the world?\\nThe largest animal in the world is the blue whale.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohere = CohereAPI()\n",
    "cohere.query(example_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nHow long do elephants live?\\n\\nMany people are fascinated by the lives of elephants, yet are unclear as to how long the African elephant's life span is. When you consider that one of the\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = GPTj6bAPI()\n",
    "gpt.query(example_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database loaders\n",
    "* openbookqa\n",
    "* hotpotqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus and train questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2afed9e6eca3466a949c7c3849b26e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "corpus.jsonl:   0%|          | 0/1326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b531dd36e204a8a921802c634e4bf97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl:   0%|          | 0/4957 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OPENBOOKQA_CORPUS_PATH ='./../assets/benchmarks/openbookqa/corpus.jsonl'\n",
    "OPENBOOKQA_TRAIN_QUESTIONS_PATH = './../assets/benchmarks/openbookqa/train.jsonl'\n",
    "\n",
    "openbookqa_database = DatabaseLoaderFactory(\"openbookqa\")\n",
    "openbookqa_corpus = openbookqa_database.load_documents(OPENBOOKQA_CORPUS_PATH)\n",
    "openbookqa_train_questions =  openbookqa_database.load_questions(OPENBOOKQA_TRAIN_QUESTIONS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A bee is a pollinating animal',\n",
       " 'A bird is a pollinating animal',\n",
       " 'An electrical conductor is a vehicle for the flow of electricity',\n",
       " 'An example of a change in the Earth is an ocean becoming a wooded area',\n",
       " 'An example of a chemical change is acid breaking down substances']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openbookqa_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '7-980',\n",
       " 'text': 'The sun is responsible for',\n",
       " 'possible_answers': 'A. puppies learning new tricks B. children growing up and getting old C. flowers wilting in a vase D. plants sprouting, blooming and wilting',\n",
       " 'answer': 'D. plants sprouting, blooming and wilting '}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openbookqa_train_questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c6504db0064546a383b93da5366aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dev.jsonl:   0%|          | 0/7405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HOTPOTQA_DEV_QUESTIONS_PATH = './../assets/benchmarks/hotpotqa/dev.jsonl'\n",
    "\n",
    "hotpotqa_database = DatabaseLoaderFactory(\"hotpotqa\")\n",
    "hotpotqa_test_questions =  hotpotqa_database.load_questions(HOTPOTQA_DEV_QUESTIONS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5a8b57f25542995d1e6f1371',\n",
       " 'text': 'Were Scott Derrickson and Ed Wood of the same nationality?',\n",
       " 'possible_answers': None,\n",
       " 'answer': 'yes'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotpotqa_test_questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information retrievers\n",
    "* BM25Retriever (databases: openbookqa, hotpotqa)\n",
    "* AnnoyRetriever (databases: openbookqa, hotpotqa)\n",
    "* FAISSRetriever (databases: openbookqa, hotpotqa)\n",
    "* GoogleCustomSearch\n",
    "* WikiAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documents retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_openbook_question = \"The sun is responsible for\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c39d9848d3648a3b8e8834ce3c3ca5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "corpus.jsonl:   0%|          | 0/1326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['seasonal changes are made in response to changes in the environment',\n",
       " 'if it is night then the sun has set',\n",
       " 'the Earth revolves around the sun',\n",
       " 'the sun sets in the west',\n",
       " 'Earth is the planet that is third closest to the Sun']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25 = BM25Retriever(k=5,\n",
    "                     database=\"openbookqa\",\n",
    "                     database_path=\"./../assets/benchmarks/openbookqa/corpus.jsonl\",\n",
    "                     database_index=\"./../assets/benchmarks/openbookqa/corpus_bm25_index.pickle\")\n",
    "bm25.get_documents(example_openbook_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e110ef4f7845d88d2fd81da26c925b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "corpus.jsonl:   0%|          | 0/1326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the sun is the source of solar energy called sunlight',\n",
       " 'the sun is a source of heat called sunlight',\n",
       " 'the sun is a source of light called sunlight',\n",
       " 'the sun is the source of energy for life on Earth',\n",
       " 'the sun is the source of energy for physical cycles on Earth']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annoy = AnnoyRetriever(k=5,\n",
    "                       database=\"openbookqa\",\n",
    "                       database_path=\"./../assets/benchmarks/openbookqa/corpus.jsonl\",\n",
    "                       database_index=\"./../assets/benchmarks/openbookqa/corpus_annoy_index_384.ann\")\n",
    "\n",
    "annoy.get_documents(example_openbook_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3e5db969034e409311d51a197d0923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "corpus.jsonl:   0%|          | 0/1326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['the sun is the source of solar energy called sunlight',\n",
       " 'the sun is a source of heat called sunlight',\n",
       " 'the sun is a source of light called sunlight',\n",
       " 'the sun is the source of energy for life on Earth',\n",
       " 'the sun is the source of energy for physical cycles on Earth']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faiss = FAISSRetriever(k=5,\n",
    "                       database=\"openbookqa\",\n",
    "                       database_path=\"./../assets/benchmarks/openbookqa/corpus.jsonl\",\n",
    "                       database_index=\"./../assets/benchmarks/openbookqa/corpus_faiss_index.pickle\")\n",
    "faiss.get_documents(example_openbook_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_opendomain_question = \"Who is the best chess player in the world?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Several methods have been suggested for comparing the greatest chess players in history. There is agreement on a statistical system to rate the strengths of current players, called the Elo system, but disagreement about methods used to compare players from different generations who never competed against each other.\\n\\nStatistical methods [ edit ]\\n\\nElo system [ edit ]\\n\\nThe best-known statistical method was devised by Arpad Elo in 1960 and elaborated on in his 1978 book The Rating of Chessplayers, Past and Present.[1] He gave ratings to players corresponding to their performance over the best five-year span of their career. According to this system the highest ratings achieved were:\\n\\nThough published in 1978, Elo's list did not include five-year averages for later players Bobby Fischer and Anatoly Karpov. It did list January 1978 ratings of 2780 for Fischer and 2725 for Karpov.[2]\\n\\nIn 1970, FIDE adopted Elo's system for rating current players, so one way to compare players of different er\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcs = GoogleCustomSearch(k=1)\n",
    "gcs.get_documents(example_opendomain_question)[0][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Several methods have been suggested for comparing the greatest chess players in history. There is agreement on a statistical system to rate the strengths of current players, called the Elo system, but disagreement about methods used to compare players from different generations who never competed against each other.\\n\\n\\n== Statistical methods ==\\n\\n\\n=== Elo system ===\\n\\nThe best-known statistical method was devised by Arpad Elo in 1960 and elaborated on in his 1978 book The Rating of Chessplayers, Past and Present. He gave ratings to players corresponding to their performance over the best five-year span of their career. According to this system the highest ratings achieved were:\\n\\n2725: José Raúl Capablanca\\n2720: Mikhail Botvinnik, Emanuel Lasker\\n2700: Mikhail Tal\\n2690: Alexander Alekhine, Paul Morphy, Vasily SmyslovThough published in 1978, Elo's list did not include five-year averages for later players Bobby Fischer and Anatoly Karpov. It did list January 1978 ratings of 2780 for Fischer and 2725 for Karpov.In 1970, FIDE adopted Elo's system for rating current players, so one way to compare players of different eras is to compare their Elo ratings. The best-ever Elo ratings are tabulated below.As of December 2015, there were 101 chess players in history who broke 2700, and fourteen of them exceeded 2800. The high peak ratings of Fischer, Karpov, and Kasparov are notable for being achieved last century (1972, 1994, and 1999 respectively). However, Fischer and Karpov are no longer in the top 20.\\n\\n\\n=== Average rating over time ===\\nThe average Elo rating of top players has risen over time.\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki = WikiAPI(k=10)\n",
    "wiki.get_documents(example_opendomain_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIQARD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration with prepared prompt template and without information retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack Dorsey is the current CEO of Twitter. He was appointed to the position in October 2015.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate('./../assets/prompting_templates/like_chat_gpt/without_context.txt')\n",
    "llm = CohereAPI(stop_token=\"\\n\")\n",
    "\n",
    "piqard = PIQARD(prompt_template=prompt_template,\n",
    "                language_model=llm)\n",
    "\n",
    "result = piqard(\"Who is the current CEO of twitter?\")\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m[base_prompt] Assistant is a large language model.\n",
      "\n",
      "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
      "\n",
      "Human: Who is the current CEO of twitter?\n",
      "Assistant:\n",
      "\u001b[0m\u001b[34m[thought] Jack Dorsey is the current CEO of Twitter. He was appointed to the position in October 2015.\n",
      "\u001b[0m\u001b[32m[finish] Jack Dorsey is the current CEO of Twitter. He was appointed to the position in October 2015.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(result['chain_trace'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration with prepared prompt template and with information retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk is the current CEO of Twitter.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate('./../assets/prompting_templates/like_chat_gpt/with_context.txt')\n",
    "llm = CohereAPI(stop_token=\"\\n\")\n",
    "ir = WikiAPI(k=10)\n",
    "\n",
    "piqard = PIQARD(prompt_template=prompt_template,\n",
    "                language_model=llm,\n",
    "                information_retriever=ir)\n",
    "\n",
    "result = piqard(\"Who is the current CEO of twitter?\")\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[observation] Twitter, Inc. is an American social media company based in San Francisco, California. The company operates the microblogging and social networking service Twitter. It previously operated the Vine short video app and Periscope livestreaming service.\n",
      "Twitter was created by Jack Dorsey, Noah Glass, Biz Stone, and Evan Williams in March 2006 and was launched that July. By 2012, more than 100 million users tweeted 340 million tweets a day, and the service handled an average of 1.6 billion search queries per day. The company went public in November 2013. By 2019, Twitter had more than 330 million monthly active users.On April 25, 2022, Twitter agreed to a $44 billion buyout by Elon Musk, CEO of SpaceX and Tesla, one of the biggest deals to turn a company private. On July 8, Musk terminated the deal. Twitter's shares fell, leading company officials to sue Musk in the Chancery Court of Delaware on July 12.\n",
      "\u001b[0m\u001b[37m[base_prompt] Assistant is a large language model.\n",
      "\n",
      "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Assistant may receive additional context from the user indicated by \"Context:\". It is using the received context to provide a more accurate and helpful response.\n",
      "\n",
      "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
      "\n",
      "Human: Who is the current CEO of twitter? Context: Twitter, Inc. is an American social media company based in San Francisco, California. The company operates the microblogging and social networking service Twitter. It previously operated the Vine short video app and Periscope livestreaming service.\n",
      "Twitter was created by Jack Dorsey, Noah Glass, Biz Stone, and Evan Williams in March 2006 and was launched that July. By 2012, more than 100 million users tweeted 340 million tweets a day, and the service handled an average of 1.6 billion search queries per day. The company went public in November 2013. By 2019, Twitter had more than 330 million monthly active users.On April 25, 2022, Twitter agreed to a $44 billion buyout by Elon Musk, CEO of SpaceX and Tesla, one of the biggest deals to turn a company private. On July 8, Musk terminated the deal. Twitter's shares fell, leading company officials to sue Musk in the Chancery Court of Delaware on July 12.\n",
      "Assistant:\n",
      "\u001b[0m\u001b[34m[thought] Elon Musk is the current CEO of Twitter.\n",
      "\u001b[0m\u001b[32m[finish] Elon Musk is the current CEO of Twitter.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(result['chain_trace'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration with custom prompt template and with information retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Answer the question based on the context.\n",
    "\n",
    "Context: {% if context %}{{\" \".join(context)}}{% endif %}\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lake Pontchartrain Causeway, Louisiana, USA.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(template)\n",
    "llm = CohereAPI(stop_token=\"\\n\")\n",
    "ir = WikiAPI(k=10)\n",
    "\n",
    "piqard = PIQARD(prompt_template=prompt_template,\n",
    "                language_model=llm,\n",
    "                information_retriever=ir)\n",
    "\n",
    "result = piqard(\"What is the longest bridge in the USA?\")\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[observation] The Longest Day is a 1962 American epic war film, shot in black and white and based on Cornelius Ryan's 1959 non-fiction book of the same name about the D-Day landings at Normandy on June 6, 1944. The film was produced by Darryl F. Zanuck, who paid author Ryan $175,000 for the film rights. The screenplay was by Ryan, with additional material written by Romain Gary, James Jones, David Pursall, and Jack Seddon. It was directed by Ken Annakin (British and French exteriors), Andrew Marton (American exteriors), and Bernhard Wicki (German scenes).\n",
      "The Longest Day features a large international ensemble cast including John Wayne, Kenneth More, Richard Todd, Robert Mitchum, Richard Burton, Steve Forrest, Sean Connery, Henry Fonda, Red Buttons, Peter Lawford, Eddie Albert, Jeffrey Hunter, Stuart Whitman, Tom Tryon, Rod Steiger, Leo Genn, Gert Fröbe, Irina Demick, Bourvil, Curd Jürgens, George Segal, Robert Wagner, Paul Anka, and Arletty. Many of these actors played roles that were essentially cameo appearances. In addition, several cast members had seen action as servicemen during the war, including Albert, Fonda, Genn, More, Steiger, and Todd; Todd was among the first British officers to land in Normandy in Operation Overlord, and he participated in the assault on Pegasus Bridge.\n",
      "The film employed several Axis and Allied military consultants who had been actual participants on D-Day, and many had their roles re-enacted in the film. These included Günther Blumentritt (a former German general), James M. Gavin (an American general), Frederick Morgan (Deputy Chief of Staff at SHAEF), John Howard (who led the airborne assault on the Pegasus Bridge), Lord Lovat (who commanded the 1st Special Service Brigade), Philippe Kieffer (who led his men in the assault on Ouistreham), Marie-Pierre Kœnig (who commanded the Free French Forces in the invasion), Max Pemsel (a German general), Werner Pluskat (the major who was the first German officer to see the invasion fleet), Josef \"Pips\" Priller (the hot-headed pilot), and Lucie Rommel (widow of Field Marshal Erwin Rommel).\n",
      "The film won two Academy Awards and was nominated for three others.\n",
      "\u001b[0m\u001b[37m[base_prompt] Answer the question based on the context.\n",
      "\n",
      "Context: The Longest Day is a 1962 American epic war film, shot in black and white and based on Cornelius Ryan's 1959 non-fiction book of the same name about the D-Day landings at Normandy on June 6, 1944. The film was produced by Darryl F. Zanuck, who paid author Ryan $175,000 for the film rights. The screenplay was by Ryan, with additional material written by Romain Gary, James Jones, David Pursall, and Jack Seddon. It was directed by Ken Annakin (British and French exteriors), Andrew Marton (American exteriors), and Bernhard Wicki (German scenes).\n",
      "The Longest Day features a large international ensemble cast including John Wayne, Kenneth More, Richard Todd, Robert Mitchum, Richard Burton, Steve Forrest, Sean Connery, Henry Fonda, Red Buttons, Peter Lawford, Eddie Albert, Jeffrey Hunter, Stuart Whitman, Tom Tryon, Rod Steiger, Leo Genn, Gert Fröbe, Irina Demick, Bourvil, Curd Jürgens, George Segal, Robert Wagner, Paul Anka, and Arletty. Many of these actors played roles that were essentially cameo appearances. In addition, several cast members had seen action as servicemen during the war, including Albert, Fonda, Genn, More, Steiger, and Todd; Todd was among the first British officers to land in Normandy in Operation Overlord, and he participated in the assault on Pegasus Bridge.\n",
      "The film employed several Axis and Allied military consultants who had been actual participants on D-Day, and many had their roles re-enacted in the film. These included Günther Blumentritt (a former German general), James M. Gavin (an American general), Frederick Morgan (Deputy Chief of Staff at SHAEF), John Howard (who led the airborne assault on the Pegasus Bridge), Lord Lovat (who commanded the 1st Special Service Brigade), Philippe Kieffer (who led his men in the assault on Ouistreham), Marie-Pierre Kœnig (who commanded the Free French Forces in the invasion), Max Pemsel (a German general), Werner Pluskat (the major who was the first German officer to see the invasion fleet), Josef \"Pips\" Priller (the hot-headed pilot), and Lucie Rommel (widow of Field Marshal Erwin Rommel).\n",
      "The film won two Academy Awards and was nominated for three others.\n",
      "Question: What is the longest bridge in the USA?\n",
      "Answer:\n",
      "\u001b[0m\u001b[34m[thought] The Lake Pontchartrain Causeway, Louisiana, USA.\n",
      "\u001b[0m\u001b[32m[finish] The Lake Pontchartrain Causeway, Louisiana, USA.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(result['chain_trace'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration with prepared prompt template, without information retrieval and with possible answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A. fruit\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(template='./../assets/prompting_templates/openbookqa/chain_of_thought/cot_3_shot.txt',\n",
    "                                 fix_text=\"So the final answer is:\")\n",
    "llm = CohereAPI(stop_token=\"|||\")\n",
    "\n",
    "piqard = PIQARD(language_model=llm,\n",
    "                prompt_template=prompt_template)\n",
    "\n",
    "result = piqard(\"A cactus stem is used to store\", \"A. fruit B.liquid C. food D. spines\")\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m[base_prompt] Answer the question\n",
      "\n",
      "Question: The sun is responsible for\n",
      "Possible answers: A. puppies learning new tricks, B. children growing up and getting old, C. flowers wilting in a vase, D. plants sprouting, blooming and wilting\n",
      "Let's think step by step:\n",
      "- puppies can learn new tricks even in the dark or inside a house\n",
      "- children growing up and getting old is a natural cycle of life\n",
      "- flowers wilting in a vase could result from not suppyling enough water\n",
      "- sunlight is essential for plants for photosynthesis which make them grow, sprout, bloom and wilt when there's not enough of it\n",
      "So the final answer is: D. plants sprouting, blooming and wilting\n",
      "|||\n",
      "\n",
      "Question: A magnet will stick to\n",
      "Possible answers: A. a belt buckle, B. a wooden table, C. a plastic cup, D. a paper plate\n",
      "Let's think step by step:\n",
      "- a magnet will attract magnetic metals through magnetism\n",
      "- a belt buckle is usually made out of metal such as nickel, zinc or copper alloys which are magnetic\n",
      "- wood, plastic or paper are materials that do not contain metal and thus are not magnetic\n",
      "So the final answer is: A. a belt buckle\n",
      "|||\n",
      "\n",
      "Question: Which would be a logical hypothesis after viewing a white substance on the floor, with a yellow carton on the counter?\n",
      "Possible answers: A. milk was spilled, B. white juice was spilled, C. bleach was spilled, D. was an illusion\n",
      "Let's think step by step:\n",
      "- hypothesis means scientific guess about the cause and effect of an event\n",
      "- the yellow carton on the counter was most likely found in the kitchen\n",
      "- milk is a white liquid which is usually stored in carton boxes\n",
      "- white juice is not very specific\n",
      "- bleach is a white liquid but it's unlikely to be found on a counter\n",
      "- let's not consider the possiblity of being mistaken about seeing the white substance on the floor and it being just an illusion\n",
      "So the final answer is: A. milk was spilled\n",
      "|||\n",
      "\n",
      "Question: A cactus stem is used to store\n",
      "Possible answers: A. fruit B.liquid C. food D. spines\n",
      "Let's think step by step:\n",
      "\u001b[0m\u001b[34m[thought] - cactus is a plant that stores water in its stem\n",
      "- fruit is the part of a plant that contains seeds\n",
      "- liquid is a substance that flows freely and has no shape\n",
      "- food is a substance that provides nutrition to living organisms- spines are sharp, needle-like structures found on some plants\n",
      "So the final answer is: A. fruit\n",
      "\n",
      "\u001b[0m\u001b[32m[finish] A. fruit\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(result['chain_trace'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration with prepared prompt template, without information retrieval and with possible answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c2ec15e37e470aa4763c70a64b3801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "corpus.jsonl:   0%|          | 0/1326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B.liquid\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(template='./../assets/prompting_templates/openbookqa/k_documents/5_shot_2_documents.txt',\n",
    "                                 fix_text=\"So the final answer is:\")\n",
    "llm = BLOOM176bAPI(stop_token=\"\\n\")\n",
    "ir = AnnoyRetriever(k=2,\n",
    "                    database=\"openbookqa\",\n",
    "                    database_path=\"./../assets/benchmarks/openbookqa/corpus.jsonl\",\n",
    "                    database_index=\"./../assets/benchmarks/openbookqa/corpus_annoy_index_384.ann\") \n",
    "\n",
    "piqard = PIQARD(language_model=llm,\n",
    "                information_retriever=ir,\n",
    "                prompt_template=prompt_template)\n",
    "\n",
    "result = piqard(\"A cactus stem is used to store\", \"A. fruit B.liquid C. food D. spines\")\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[observation] a cactus stem is used for storing water a cactus stores water\n",
      "\u001b[0m\u001b[37m[base_prompt] Answer the question based on the facts.\n",
      "\n",
      "Question: The sun is responsible for\n",
      "Possible answers: A. puppies learning new tricks, B. children growing up and getting old, C. flowers wilting in a vase, D. plants sprouting, blooming and wilting\n",
      "Facts:\n",
      "- the sun is the source of energy for physical cycles on Earth\n",
      "- the sun is the source of energy for life on Earth\n",
      "Answer: D. plants sprouting, blooming and wilting\n",
      "\n",
      "Question: with which could you tell the exact size of an object?\n",
      "Possible answers: A. a plain stick with irregular shape, B. a plastic tape with graduated markings, C. a thermometer with mercury in it, D. a metal cooking spoon\n",
      "Facts:\n",
      "- a tape measure is used to measure length\n",
      "- a ruler is used for measuring the length of an object\n",
      "Answer: B. a plastic tape with graduated markings\n",
      "\n",
      "Question: When food is reduced in the stomach\n",
      "Possible answers: A. the mind needs time to digest, B. take a second to digest what I said, C. nutrients are being deconstructed, D. reader's digest is a body of works\n",
      "Facts:\n",
      "- digestion is when stomach acid breaks down food\n",
      "- the breaking down of food into simple substances occurs in the digestive system\n",
      "Answer: C. nutrients are being deconstructed\n",
      "\n",
      "Question: Stars are\n",
      "Possible answers: A. warm lights that float, B. made out of nitrate, C. great balls of gas burning billions of miles away, D. lights in the sky\n",
      "Facts:\n",
      "- a star is made of gases\n",
      "- the stars in the night sky are very far away from the Earth\n",
      "Answer: C. great balls of gas burning billions of miles away\n",
      "\n",
      "Question: Poison causes harm to which of the following?\n",
      "Possible answers:  A. a tree, B. a robot, C. a house, D. a car\n",
      "Facts:\n",
      "- poison causes harm to living things\n",
      "- harming something has a negative effect on that something\n",
      "Answer: A. a tree\n",
      "\n",
      "Question: A cactus stem is used to store\n",
      "Possible answers: A. fruit B.liquid C. food D. spines\n",
      "Facts: \n",
      "- a cactus stem is used for storing water\n",
      "- a cactus stores water\n",
      "Answer:\n",
      "\u001b[0m\u001b[34m[thought]  B.liquid\n",
      "\u001b[0m\u001b[32m[finish] B.liquid\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(result['chain_trace'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### React"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(template='./../assets/prompting_templates/react/react_prompt.txt')\n",
    "llm = BLOOM176bAPI(stop_token=\"\\n\", temperature=1, top_k=1)\n",
    "faiss = FAISSRetriever(k=1,\n",
    "                       database=\"hotpotqa\",\n",
    "                       database_path=\"./../assets/benchmarks/hotpotqa/corpus.jsonl\",\n",
    "                       database_index=\"./../assets/benchmarks/hotpotqa/corpus_faiss_index.pickle\")\n",
    "\n",
    "actions = [\n",
    "    Action(\"Wikipedia\", faiss, prefix=\"Search\")\n",
    "]\n",
    "\n",
    "react_agent = Agent(prompt_template=prompt_template,\n",
    "                    language_model=llm,\n",
    "                    actions=actions)\n",
    "\n",
    "result = react_agent(\"the work of The Last Poets and Gil Scott Heron inspired music that fits under what larger sub-genre?\")\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['chain_trace'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelfAware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if should not browse\n",
    "piqard_prompt_template = PromptTemplate('./../assets/prompting_templates/like_chat_gpt/without_context.txt')\n",
    "piqard_llm = CohereAPI(stop_token=\"\\n\")\n",
    "\n",
    "piqard = PIQARD(prompt_template=piqard_prompt_template,\n",
    "                language_model=piqard_llm,\n",
    "                information_retriever=None)\n",
    "\n",
    "# if should browse\n",
    "react_prompt_template = PromptTemplate('./../assets/prompting_templates/react/react_prompt.txt')\n",
    "react_llm = CohereAPI(stop_token=\"\\n\")\n",
    "\n",
    "actions = [\n",
    "    Action(\"Wikipedia\", WikiAPI(k=5), prefix=\"Search\")\n",
    "]\n",
    "\n",
    "react_agent = Agent(prompt_template=react_prompt_template,\n",
    "                    language_model=react_llm,\n",
    "                    actions=actions)\n",
    "\n",
    "# selfAware\n",
    "self_aware_prompt_template = PromptTemplate('./../assets/prompting_templates/self_aware/self_aware_prompt.txt')\n",
    "sefl_aware_llm = CohereAPI(stop_token=\"\\n\")\n",
    "\n",
    "self_aware = SelfAware(prompt_template=self_aware_prompt_template,\n",
    "                       language_model=sefl_aware_llm,\n",
    "                       if_should_browse=react_agent,\n",
    "                       if_should_not_browse=piqard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yellow is a color that is often associated with happiness and optimism. It is also the color of the sun, which is often seen as a symbol of hope and new beginnings.\n",
      "\u001b[34m[thought] Should I browse the web for an answer?: no\n",
      "\u001b[0m\u001b[37m[base_prompt] Assistant is a large language model.\n",
      "\n",
      "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
      "\n",
      "Human: What is yellow color?\n",
      "Assistant:\n",
      "\u001b[0m\u001b[34m[thought] Yellow is a color that is often associated with happiness and optimism. It is also the color of the sun, which is often seen as a symbol of hope and new beginnings.\n",
      "\u001b[0m\u001b[32m[finish] Yellow is a color that is often associated with happiness and optimism. It is also the color of the sun, which is often seen as a symbol of hope and new beginnings.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = self_aware(\"What is yellow color?\")\n",
    "print(result['answer'])\n",
    "print(result['chain_trace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack Dorsey\n",
      "\n",
      "\u001b[34m[thought] Should I browse the web for an answer?: yes\n",
      "\u001b[0m\u001b[37m[base_prompt] Question: What profession does Nicholas Ray and Elia Kazan have in common?\n",
      "Thought 1: I need to search Nicholas Ray and Elia Kazan, find their professions, then find the profession they have in common.\n",
      "Action 1: Search[Nicholas Ray]\n",
      "Observation: Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16, 1979) was an American film director, screenwriter, and actor best known for the 1955 film Rebel Without a Cause.\n",
      "Thought 2: Professions of Nicholas Ray are director, screenwriter, and actor. I need to search Elia Kazan next and find his professions.\n",
      "Action 2: Search[Elia Kazan]\n",
      "Observation: Elia Kazan was an American film and theatre director, producer, screenwriter and actor.\n",
      "Thought 3: Professions of Elia Kazan are director, producer, screenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in common is director, screenwriter, and actor.\n",
      "Action 3: Finish[director, screenwriter, actor]\n",
      "---\n",
      "Question: Which magazine was started first Arthur's Magazine or First for Women?\n",
      "Thought 1: I need to search Arthur's Magazine and First for Women, and find which was started first.\n",
      "Action 1: Search[Arthur's Magazine]\n",
      "Observation: Arthur's Magazine (1844-1846) was an American literary periodical published in Philadelphia in the 19th century.\n",
      "Thought 2: Arthur's Magazine was started in 1844. I need to search First for Women next.\n",
      "Action 2: Search[First for Women]\n",
      "Observation: First for Women is a woman's magazine published by Bauer Media Group in the USA.[1] The magazine was started in 1989.\n",
      "Thought 3: First for Women was started in 1989. 1844 < 1989, so Arthur's Magazine was started first.\n",
      "Action 3: Finish[Arthur's Magazine]\n",
      "---\n",
      "Question: What is the longest bridge in the world?\n",
      "Thought 1: I need to search the name of the longest bridge in the world.\n",
      "Action 1: Search[the longest bridge in the world]\n",
      "Observation 1: The Danyang-Kunshan Grand Bridge is officially the longest bridge in the world. It stretches out for no less than 102.4 miles (164.8 kilometres) and forms part of one of China's most important railway connections: the high-speed train between Beijing and Shanghai.\n",
      "Action 2: Finish[The Danyang-Kunshan Grand Bridge]\n",
      "---\n",
      "Question: Were Pavel Urysohn and Leonid Levin known for the same type of work?\n",
      "Thought 1: I need to search Pavel Urysohn and Leonid Levin, find their types of work, then find if they are the same.\n",
      "Action 1: Search[Pavel Urysohn]\n",
      "Observation: Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet mathematician who is best known for his contributions in dimension theory.\n",
      "Thought 2: Pavel Urysohn is a mathematician. I need to search Leonid Levin next and find its type of work.\n",
      "Action 2: Search[Leonid Levin]\n",
      "Observation: Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist.\n",
      "Thought 3: Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn and Leonid Levin have the same type of work.\n",
      "Action 3: Finish[yes]\n",
      "---\n",
      "Question: Who is the current CEO of twitter?\n",
      "\u001b[0m\u001b[34m[thought] Thought 1: I need to search the current CEO of twitter.\n",
      "\u001b[0m\u001b[31m[action] Action 1: Search[the current CEO of twitter]\n",
      "\u001b[0m\u001b[33m[observation] Observation: Twitter, Inc. is an American social media company based in San Francisco, California. The company operates the microblogging and social networking service Twitter. It previously operated the Vine short video app and Periscope livestreaming service.\n",
      "Twitter was created by Jack Dorsey, Noah Glass, Biz Stone, and Evan Williams in March 2006 and was launched that July.\n",
      "\u001b[0m\u001b[34m[thought] Thought 2: Jack Dorsey is the current CEO of twitter. I need to search the previous CEO of twitter next.\n",
      "\u001b[0m\u001b[31m[action] Action 2: Search[the previous CEO of twitter]\n",
      "\u001b[0m\u001b[33m[observation] Observation: Could not find the previous CEO of twitter. Similar: ['Twitter, Inc.', 'Acquisition of Twitter by Elon Musk', 'Twitter Files', 'Twitter', 'Elon Musk'].\n",
      "\u001b[0m\u001b[34m[thought] Thought 3: Could not find the previous CEO of twitter. So Jack Dorsey is the current CEO of twitter.\n",
      "\u001b[0m\u001b[31m[action] Action 3: Finish[Jack Dorsey]\n",
      "\u001b[0m\u001b[32m[finish] Jack Dorsey\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = self_aware(\"Who is the current CEO of twitter?\")\n",
    "print(result['answer'])\n",
    "print(result['chain_trace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
