{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"COHERE_API_KEY\"] = \"eYDbpsxzTril5NSJTGhv3olRwtNuuAkl9WHK5Vl5\"\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
    "os.environ[\"GOOGLE_CUSTOM_SEARCH_API_KEY\"] = \"AIzaSyAB46rrYmTj6_w-7qCME3Gve7vqcUGzwAY\"\n",
    "os.environ[\"GOOGLE_CUSTOM_SEARCH_ENGINE_ID\"] = \"21b53499491814de3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from piqard.utils.prompt_template import PromptTemplate\n",
    "from piqard.data_loaders import DatabaseLoaderFactory\n",
    "from piqard.information_retrievers import BM25Retriever, AnnoyRetriever, FAISSRetriever, GoogleCustomSearch, WikiAPI\n",
    "from piqard.language_models import CohereAPI, BLOOM176bAPI, GPTj6bAPI\n",
    "from piqard.PIQARD import PIQARD\n",
    "from piqard.extensions.react import Agent, Action\n",
    "from piqard.extensions.self_aware import SelfAware\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Language Models\n",
    "* CohereAPI\n",
    "* BLOOM176bAPI\n",
    "* GPTJ6bAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cohere = CohereAPI(stop_token=\"\\n\")\n",
    "bloom = BLOOM176bAPI(stop_token=\"\\n\")\n",
    "gpt = GPTj6bAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available database loaders\n",
    "* openbookqa\n",
    "* hotpotqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "corpus.jsonl: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1326/1326 [00:00<00:00, 444647.19it/s]\n",
      "train.jsonl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4957/4957 [00:00<00:00, 147099.32it/s]\n"
     ]
    }
   ],
   "source": [
    "OPENBOOKQA_CORPUS_PATH ='./../assets/benchmarks/openbookqa/corpus.jsonl'\n",
    "OPENBOOKQA_TRAIN_QUESTIONS_PATH = './../assets/benchmarks/openbookqa/train.jsonl'\n",
    "\n",
    "openbookqa_database = DatabaseLoaderFactory(\"openbookqa\")\n",
    "openbookqa_corpus = openbookqa_database.load_documents(OPENBOOKQA_CORPUS_PATH)\n",
    "openbookqa_train_questions =  openbookqa_database.load_questions(OPENBOOKQA_TRAIN_QUESTIONS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev.jsonl: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7405/7405 [00:00<00:00, 21595.73it/s]\n"
     ]
    }
   ],
   "source": [
    "HOTPOTQA_DEV_QUESTIONS_PATH = './../assets/benchmarks/hotpotqa/dev.jsonl'\n",
    "\n",
    "hotpotqa_database = DatabaseLoaderFactory(\"hotpotqa\")\n",
    "hotpotqa_test_questions =  hotpotqa_database.load_questions(HOTPOTQA_DEV_QUESTIONS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information retrievers\n",
    "* BM25Retriever (databases: openbookqa, hotpotqa)\n",
    "* AnnoyRetriever (databases: openbookqa, hotpotqa)\n",
    "* FAISSRetriever (databases: openbookqa, hotpotqa)\n",
    "* GoogleCustomSearch\n",
    "* WikiAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "corpus.jsonl: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1326/1326 [00:00<00:00, 369593.77it/s]\n",
      "corpus.jsonl: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1326/1326 [00:00<?, ?it/s]\n",
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "corpus.jsonl: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1326/1326 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "bm25 = BM25Retriever(database=\"openbookqa\",\n",
    "                     database_path=\"./../assets/benchmarks/openbookqa/corpus.jsonl\",\n",
    "                     database_index=\"./../assets/benchmarks/openbookqa/bm25_index.pickle\")\n",
    "\n",
    "annoy = AnnoyRetriever(database=\"openbookqa\",\n",
    "                       database_path=\"./../assets/benchmarks/openbookqa/corpus.jsonl\",\n",
    "                       database_index=\"./../assets/benchmarks/openbookqa/annoy_index_384.ann\")\n",
    "\n",
    "faiss = FAISSRetriever(database=\"openbookqa\",\n",
    "                       database_path=\"./../assets/benchmarks/openbookqa/corpus.jsonl\",\n",
    "                       database_index=\"./../assets/benchmarks/openbookqa/faiss_index.pickle\")\n",
    "\n",
    "gcs = GoogleCustomSearch()\n",
    "\n",
    "wiki = WikiAPI(k=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIQARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama is the 44th president of the United States. He is the first African-American to hold the office. Obama previously served as a United States senator from Illinois, from January 2005 to November 2008.\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate('./../assets/prompting_templates/like_chat_gpt/with_context.txt')\n",
    "llm = CohereAPI(stop_token=\"\\n\")\n",
    "ir = WikiAPI(k=10)\n",
    "\n",
    "piqard = PIQARD(prompt_template=prompt_template,\n",
    "                language_model=llm,\n",
    "                information_retriever=ir)\n",
    "\n",
    "result = piqard(\"Who is Barack Obama?\")\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[observation] The family of Barack Obama, the 44th president of the United States, is a prominent American family active in law, education, activism and politics. Obama's immediate family circle was the first family of the United States from 2009 to 2017, and are the first such family of African-American descent. His immediate family includes his wife Michelle Obama and daughters Malia and Sasha Obama.\n",
      "Obama's wider ancestry is made up of people of Kenyan (Luo), African-American, and Old Stock American (including originally English, Scots-Irish, Welsh, German, and Swiss) ancestry.\n",
      "\n",
      "\n",
      "== Immediate family ==\n",
      "\n",
      "\n",
      "=== Michelle Obama ===\n",
      "\n",
      "Michelle LaVaughn Robinson Obama (born January 17, 1964) is an American lawyer, university administrator, and writer who served as the First Lady of the United States from 2009 to 2017. She is Barack Obama's wife, and was the first African-American first lady. Raised on the South Side of Chicago, Michelle Obama is a graduate of Princeton University and Harvard Law School, and spent her early legal career working at the law firm Sidley Austin, where she met her husband. She subsequently worked as the associate dean of Student Services at the University of Chicago and the vice president for Community and External Affairs of the University of Chicago Medical Center. Barack and Michelle married in 1992.Michelle campaigned for her husband's presidential bid throughout 2007 and 2008, delivering a keynote address at the 2008 Democratic National Convention. She returned to speak at the 2012 Democratic National Convention, and again during the 2016 Democratic National Convention in Philadelphia, where she delivered a speech in support of the Democratic presidential nominee, and fellow first lady, Hillary Clinton.As first lady, Michelle Obama sought to become a role model for women, an advocate for poverty awareness, education, nutrition, physical activity and healthy eating, and became a fashion icon.\n",
      "\u001b[0m\u001b[37mAssistant is a large language model.\n",
      "\n",
      "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Assistant may receive additional context from the user indicated by \"Context:\". It is using the received context to provide a more accurate and helpful response.\n",
      "\n",
      "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
      "\n",
      "Human: Who is Barack Obama? Context: The family of Barack Obama, the 44th president of the United States, is a prominent American family active in law, education, activism and politics. Obama's immediate family circle was the first family of the United States from 2009 to 2017, and are the first such family of African-American descent. His immediate family includes his wife Michelle Obama and daughters Malia and Sasha Obama.\n",
      "Obama's wider ancestry is made up of people of Kenyan (Luo), African-American, and Old Stock American (including originally English, Scots-Irish, Welsh, German, and Swiss) ancestry.\n",
      "\n",
      "\n",
      "== Immediate family ==\n",
      "\n",
      "\n",
      "=== Michelle Obama ===\n",
      "\n",
      "Michelle LaVaughn Robinson Obama (born January 17, 1964) is an American lawyer, university administrator, and writer who served as the First Lady of the United States from 2009 to 2017. She is Barack Obama's wife, and was the first African-American first lady. Raised on the South Side of Chicago, Michelle Obama is a graduate of Princeton University and Harvard Law School, and spent her early legal career working at the law firm Sidley Austin, where she met her husband. She subsequently worked as the associate dean of Student Services at the University of Chicago and the vice president for Community and External Affairs of the University of Chicago Medical Center. Barack and Michelle married in 1992.Michelle campaigned for her husband's presidential bid throughout 2007 and 2008, delivering a keynote address at the 2008 Democratic National Convention. She returned to speak at the 2012 Democratic National Convention, and again during the 2016 Democratic National Convention in Philadelphia, where she delivered a speech in support of the Democratic presidential nominee, and fellow first lady, Hillary Clinton.As first lady, Michelle Obama sought to become a role model for women, an advocate for poverty awareness, education, nutrition, physical activity and healthy eating, and became a fashion icon.\n",
      "Assistant:\n",
      "\u001b[0m\u001b[34mBarack Obama is the 44th president of the United States. He is the first African-American to hold the office. Obama previously served as a United States senator from Illinois, from January 2005 to November 2008.\n",
      "\u001b[0m\u001b[32mBarack Obama is the 44th president of the United States. He is the first African-American to hold the office. Obama previously served as a United States senator from Illinois, from January 2005 to November 2008.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(result['chain_trace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "corpus.jsonl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1326/1326 [00:00<00:00, 84834.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A. fruit\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(template='./../assets/prompting_templates/openbookqa/chain_of_thought/cot_3_shot.txt',\n",
    "                                 fix_text=\"So the final answer is:\")\n",
    "llm = CohereAPI(stop_token=\"|||\")\n",
    "ir = AnnoyRetriever(database=\"openbookqa\",\n",
    "                    database_path=\"./../assets/benchmarks/openbookqa/corpus.jsonl\",\n",
    "                    database_index=\"./../assets/benchmarks/openbookqa/annoy_index_384.ann\") \n",
    "\n",
    "piqard = PIQARD(language_model=llm,\n",
    "                information_retriever=ir,\n",
    "                prompt_template=prompt_template)\n",
    "\n",
    "result = piqard(\"A cactus stem is used to store\", \"A. fruit B.liquid C. food D. spines\")\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### React"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"Question: What profession does Nicholas Ray and Elia Kazan have in common?\\nThought 1: I need to search Nicholas Ray and Elia Kazan, find their professions, then find the profession they have in common.\\nAction 1: Search[Nicholas Ray]\\nObservation: Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16, 1979) was an American film director, screenwriter, and actor best known for the 1955 film Rebel Without a Cause.\\nThought 2: Professions of Nicholas Ray are director, screenwriter, and actor. I need to search Elia Kazan next and find his professions.\\nAction 2: Search[Elia Kazan]\\nObservation: Elia Kazan was an American film and theatre director, producer, screenwriter and actor.\\nThought 3: Professions of Elia Kazan are director, producer, screenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in common is director, screenwriter, and actor.\\nAction 3: Finish[director, screenwriter, actor]\\n---\\nQuestion: Which magazine was started first Arthur's Magazine or First for Women?\\nThought 1: I need to search Arthur's Magazine and First for Women, and find which was started first.\\nAction 1: Search[Arthur's Magazine]\\nObservation: Arthur's Magazine (1844-1846) was an American literary periodical published in Philadelphia in the 19th century.\\nThought 2: Arthur's Magazine was started in 1844. I need to search First for Women next.\\nAction 2: Search[First for Women]\\nObservation: First for Women is a woman's magazine published by Bauer Media Group in the USA.[1] The magazine was started in 1989.\\nThought 3: First for Women was started in 1989. 1844 < 1989, so Arthur's Magazine was started first.\\nAction 3: Finish[Arthur's Magazine]\\n---\\nQuestion: What is the longest bridge in the world?\\nThought 1: I need to search the name of the longest bridge in the world.\\nAction 1: Search[the longest bridge in the world]\\nObservation 1: The Danyang-Kunshan Grand Bridge is officially the longest bridge in the world. It stretches out for no less than 102.4 miles (164.8 kilometres) and forms part of one of China's most important railway connections: the high-speed train between Beijing and Shanghai.\\nAction 2: Finish[The Danyang-Kunshan Grand Bridge]\\n---\\nQuestion: Were Pavel Urysohn and Leonid Levin known for the same type of work?\\nThought 1: I need to search Pavel Urysohn and Leonid Levin, find their types of work, then find if they are the same.\\nAction 1: Search[Pavel Urysohn]\\nObservation: Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet mathematician who is best known for his contributions in dimension theory.\\nThought 2: Pavel Urysohn is a mathematician. I need to search Leonid Levin next and find its type of work.\\nAction 2: Search[Leonid Levin]\\nObservation: Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist.\\nThought 3: Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn and Leonid Levin have the same type of work.\\nAction 3: Finish[yes]\\n---\\nQuestion: What is yellow color?\",\n",
       " 'raw_answer': '\\nThought 1: I need to search yellow color, find its definition, then find the definition of yellow color.\\nAction 1: Search[yellow color]\\nObservation: Yellow is the color between green and orange on the spectrum of light. It is evoked by light with a dominant wavelength of roughly 575–585 nm. It is a primary color in subtractive color systems, used in painting or color printing. In the RGB color model, used to create colors on television and computer screens, yellow is a secondary color made by combining red and green at equal intensity. Carotenoids give the characteristic yellow color to autumn leaves, corn, canaries, daffodils, and lemons, as well as egg yolks, buttercups, and bananas.\\nThought 2: Yellow color is the color between green and orange on the spectrum of light.\\nAction 2: Finish[yellow color]\\nyellow color\\n',\n",
       " 'answer': 'yellow color\\n',\n",
       " 'context': None,\n",
       " 'prompt_examples': None,\n",
       " 'chain_trace': [{'type': 'base_prompt',\n",
       "   'data': \"Question: What profession does Nicholas Ray and Elia Kazan have in common?\\nThought 1: I need to search Nicholas Ray and Elia Kazan, find their professions, then find the profession they have in common.\\nAction 1: Search[Nicholas Ray]\\nObservation: Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16, 1979) was an American film director, screenwriter, and actor best known for the 1955 film Rebel Without a Cause.\\nThought 2: Professions of Nicholas Ray are director, screenwriter, and actor. I need to search Elia Kazan next and find his professions.\\nAction 2: Search[Elia Kazan]\\nObservation: Elia Kazan was an American film and theatre director, producer, screenwriter and actor.\\nThought 3: Professions of Elia Kazan are director, producer, screenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in common is director, screenwriter, and actor.\\nAction 3: Finish[director, screenwriter, actor]\\n---\\nQuestion: Which magazine was started first Arthur's Magazine or First for Women?\\nThought 1: I need to search Arthur's Magazine and First for Women, and find which was started first.\\nAction 1: Search[Arthur's Magazine]\\nObservation: Arthur's Magazine (1844-1846) was an American literary periodical published in Philadelphia in the 19th century.\\nThought 2: Arthur's Magazine was started in 1844. I need to search First for Women next.\\nAction 2: Search[First for Women]\\nObservation: First for Women is a woman's magazine published by Bauer Media Group in the USA.[1] The magazine was started in 1989.\\nThought 3: First for Women was started in 1989. 1844 < 1989, so Arthur's Magazine was started first.\\nAction 3: Finish[Arthur's Magazine]\\n---\\nQuestion: What is the longest bridge in the world?\\nThought 1: I need to search the name of the longest bridge in the world.\\nAction 1: Search[the longest bridge in the world]\\nObservation 1: The Danyang-Kunshan Grand Bridge is officially the longest bridge in the world. It stretches out for no less than 102.4 miles (164.8 kilometres) and forms part of one of China's most important railway connections: the high-speed train between Beijing and Shanghai.\\nAction 2: Finish[The Danyang-Kunshan Grand Bridge]\\n---\\nQuestion: Were Pavel Urysohn and Leonid Levin known for the same type of work?\\nThought 1: I need to search Pavel Urysohn and Leonid Levin, find their types of work, then find if they are the same.\\nAction 1: Search[Pavel Urysohn]\\nObservation: Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet mathematician who is best known for his contributions in dimension theory.\\nThought 2: Pavel Urysohn is a mathematician. I need to search Leonid Levin next and find its type of work.\\nAction 2: Search[Leonid Levin]\\nObservation: Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist.\\nThought 3: Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn and Leonid Levin have the same type of work.\\nAction 3: Finish[yes]\\n---\\nQuestion: What is yellow color?\\n\"},\n",
       "  {'type': 'thought',\n",
       "   'data': 'Thought 1: I need to search yellow color, find its definition, then find the definition of yellow color.\\n'},\n",
       "  {'type': 'action', 'data': 'Action 1: Search[yellow color]\\n'},\n",
       "  {'type': 'observation',\n",
       "   'data': 'Observation: Yellow is the color between green and orange on the spectrum of light. It is evoked by light with a dominant wavelength of roughly 575–585 nm. It is a primary color in subtractive color systems, used in painting or color printing. In the RGB color model, used to create colors on television and computer screens, yellow is a secondary color made by combining red and green at equal intensity. Carotenoids give the characteristic yellow color to autumn leaves, corn, canaries, daffodils, and lemons, as well as egg yolks, buttercups, and bananas.\\n'},\n",
       "  {'type': 'thought',\n",
       "   'data': 'Thought 2: Yellow color is the color between green and orange on the spectrum of light.\\n'},\n",
       "  {'type': 'action', 'data': 'Action 2: Finish[yellow color]\\n'},\n",
       "  {'type': 'finish', 'data': 'yellow color\\n'}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "react_prompt_template = PromptTemplate('./../assets/prompting_templates/react/react_prompt.txt')\n",
    "actions = [\n",
    "    Action(\"Wikipedia\", WikiAPI(k=5), prefix=\"Search\")\n",
    "]\n",
    "\n",
    "react_agent = Agent(prompt_template=react_prompt_template, language_model=cohere, actions=actions)\n",
    "\n",
    "result = react_agent(\"What is yellow color?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelfAware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cohere' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# if should not browse\u001b[39;00m\n\u001b[0;32m      2\u001b[0m piqard_prompt_template \u001b[38;5;241m=\u001b[39m PromptTemplate(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./../assets/prompting_templates/like_chat_gpt/without_context.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m piqard \u001b[38;5;241m=\u001b[39m PIQARD(prompt_template\u001b[38;5;241m=\u001b[39mpiqard_prompt_template,\n\u001b[1;32m----> 4\u001b[0m                 language_model\u001b[38;5;241m=\u001b[39m\u001b[43mcohere\u001b[49m,\n\u001b[0;32m      5\u001b[0m                 information_retriever\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# if should browse\u001b[39;00m\n\u001b[0;32m      8\u001b[0m react_prompt_template \u001b[38;5;241m=\u001b[39m PromptTemplate(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./../assets/prompting_templates/react/react_prompt.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cohere' is not defined"
     ]
    }
   ],
   "source": [
    "# if should not browse\n",
    "piqard_prompt_template = PromptTemplate('./../assets/prompting_templates/like_chat_gpt/without_context.txt')\n",
    "piqard = PIQARD(prompt_template=piqard_prompt_template,\n",
    "                language_model=cohere,\n",
    "                information_retriever=None)\n",
    "\n",
    "# if should browse\n",
    "react_prompt_template = PromptTemplate('./../assets/prompting_templates/react/react_prompt.txt')\n",
    "\n",
    "actions = [\n",
    "    Action(\"Wikipedia\", WikiAPI(k=5), prefix=\"Search\")\n",
    "]\n",
    "\n",
    "react_agent = Agent(prompt_template=react_prompt_template,\n",
    "                    language_model=cohere,\n",
    "                    actions=actions)\n",
    "\n",
    "# selfAware\n",
    "self_aware_prompt_template = PromptTemplate('./../assets/prompting_templates/self_aware/self_aware_prompt.txt',\n",
    "                                           fix_text=\"/n\")\n",
    "\n",
    "self_aware = SelfAware(prompt_template=self_aware_prompt_template,\n",
    "                       language_model=cohere,\n",
    "                       if_should_browse=react_agent,\n",
    "                      if_should_not_browse=piqard)\n",
    "\n",
    "\n",
    "\n",
    "result = self_aware(\"What is yellow color?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresult\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchain_trace\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "print(result['chain_trace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
