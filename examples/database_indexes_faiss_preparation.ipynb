{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load text data\n",
    "In this section, we load the HotpotQA documents, which are facts and queries. Keep in mind that you might have to change the path as well as the loading script according to your data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "dataset_name = \"hotpot\"\n",
    "\n",
    "facts_path = \"corpus.jsonl\"\n",
    "with open(facts_path) as f:\n",
    "    fact_sets = [json.loads(line) for line in f.readlines()]\n",
    "facts = [fact_set[\"text\"] for fact_set in fact_sets]\n",
    "\n",
    "facts_path = \"queries.jsonl\"\n",
    "with open(facts_path) as f:\n",
    "    questions_sets = [json.loads(line) for line in f.readlines()]\n",
    "questions = [questions_set[\"text\"] for questions_set in questions_sets]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Encode dataset\n",
    "Next up the pre-trained model is loaded and the dataset is encoded. The encoding is done on the GPU if available, however it will take a few hours and lots of RAM to process - for HotpotQA it would be around 15 GB. There is a lighter batch encoding method available below if your resources are limited."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "model_name = \"multi-qa-MiniLM-L6-cos-v1\"\n",
    "model = SentenceTransformer(model_name, device='cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/163542 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "61994d712e4c4b869ea25f8b1b3f9ac1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = model.encode(facts, device='cuda', show_progress_bar=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save embeddings\n",
    "The embeddings are saved to disk for later use."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "with open(f'emb_{dataset_name}_{model_name}.pickle', 'wb') as f:\n",
    "    pickle.dump(embeddings, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dump/load embeddings in batch files\n",
    "If you don't have enough RAM for storing the whole embedding database, the text can be encoded and saved in batches. Then the index can be also trained with batches."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "def save_batched_embs(embeddings, directory, batch_volume):\n",
    "    required_space_gb = embeddings.shape[0] * embeddings.shape[1] * 4 / batch_volume\n",
    "    batch_size =  int(np.ceil(batch_volume / (embeddings.shape[1] * 4))) # n of rows that is 1 gbs of data\n",
    "    n_batches = int(np.ceil(embeddings.shape[0] / batch_size))\n",
    "    print(f\"Required space: {required_space_gb}, {n_batches} of files with {batch_size} embeddings each and filesize {batch_volume / 1000000000} GB\")\n",
    "    batch_indexes = [(batch_size*x, batch_size*(x+1)) for x in range(n_batches)]\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    for i, (lower, upper) in tqdm(enumerate(batch_indexes)):\n",
    "        fdir = f\"{directory}/{str(i).zfill(3)}.pickle\"\n",
    "        emb_batch = embeddings[lower:upper]\n",
    "\n",
    "        with open(fdir, 'wb') as f:\n",
    "            pickle.dump(emb_batch, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_all_batched_embs(directory):\n",
    "    files = os.listdir(directory)\n",
    "    files.sort()\n",
    "    embeddings = []\n",
    "    for f in tqdm(files):\n",
    "        with open(f\"{directory}/{f}\", 'rb') as f:\n",
    "            embeddings.append(pickle.load(f))\n",
    "    return np.concatenate(embeddings)\n",
    "\n",
    "# generator returning batches of embeddings from directory\n",
    "def batched_embs_generator(directory):\n",
    "    files = os.listdir(directory)\n",
    "    files.sort()\n",
    "    for f in files:\n",
    "        with open(f\"{directory}/{f}\", 'rb') as f:\n",
    "            yield pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "model_name = \"multi-qa-MiniLM-L6-cos-v1\"\n",
    "batch_volume = 1000000000 # size in bytes\n",
    "batch_directory = f\"./emb_{dataset_name}_{model_name}_b\"\n",
    "\n",
    "save_batched_embs(embeddings, batch_directory, batch_volume)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load embeddings\n",
    "Embeddings can be loaded if they have been saved before."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(f'emb_{dataset_name}_{model_name}.pickle', 'rb') as f:\n",
    "    embeddings = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Batched version"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embeddings = load_all_batched_embs(batch_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create and train index\n",
    "We create an index for the embeddings with the default parameters. This will take a few minutes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "d = embeddings.shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "quant = faiss.IndexFlatL2(d)\n",
    "index_ivfpq = faiss.IndexIVFPQ(quant, d, 100, 8, 8)\n",
    "index_ivfpq.train(embeddings)\n",
    "index_ivfpq.add(embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train index from batched embeddings files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:46,  2.73s/it]\n"
     ]
    }
   ],
   "source": [
    "# initiate index\n",
    "d = 384 # embedding length\n",
    "quant = faiss.IndexFlatL2(d)\n",
    "index_ivfpq = faiss.IndexIVFPQ(quant, d, 100, 8, 8)\n",
    "\n",
    "# train in batches\n",
    "gen = batched_embs_generator(batch_directory)\n",
    "for i, emb_batch in tqdm(enumerate(gen)):\n",
    "    if i==0:\n",
    "        index_ivfpq.train(emb_batch)\n",
    "    index_ivfpq.add(emb_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "index_name = \"index_ivfpq\"\n",
    "with open(f'{index_name}_hotpot_{model_name}.pickle', 'wb') as f:\n",
    "    pickle.dump(index_ivfpq, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}