{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in c:\\python38\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\python38\\lib\\site-packages (from wikipedia) (4.11.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\python38\\lib\\site-packages (from wikipedia) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python38\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python38\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python38\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python38\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2021.10.8)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\python38\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.3.2.post1)\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import wikipedia\n",
    "import re\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bloom():\n",
    "    def __init__(self, api_key: str):\n",
    "        self.API_URL = \"https://api-inference.huggingface.co/models/bigscience/bloom\"\n",
    "        self.API_KEY = api_key\n",
    "        self.headers = {\"Authorization\": f\"Bearer {self.API_KEY}\"}\n",
    "\n",
    "    def query(self, payload: str) -> str:\n",
    "        response = requests.post(self.API_URL, headers=self.headers, json=payload)\n",
    "        return response.json()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"BLOOM 176b huggingface.co API\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colored(st, color): return f\"\\u001b[{30+['black', 'red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white'].index(color)}m{st}\\u001b[0m\"\n",
    "\n",
    "def search_wikipedia(query: str, num_sentences: int = 2, verbose: bool = False) -> str:\n",
    "    try:\n",
    "        return wikipedia.summary(query, sentences=num_sentences)\n",
    "    except wikipedia.exceptions.PageError as e:\n",
    "        possible_results = wikipedia.search(query)\n",
    "        if verbose:\n",
    "            for i, topic in enumerate(possible_results):\n",
    "                print(f\"{i+1}. {topic}\")\n",
    "        return wikipedia.summary(possible_results[0], sentences=num_sentences)\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        # print(e.options)\n",
    "        # possible_results = wikipedia.search(query)\n",
    "        if verbose:\n",
    "            for i, topic in enumerate(e.options):\n",
    "                print(f\"{i+1}. {topic}\")\n",
    "        return wikipedia.summary(e.options[0], sentences=num_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChainTrace, Action, Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainTrace:\n",
    "  __type_of_nodes = ['base_prompt', 'thought', 'action', 'observation', 'finish']\n",
    "  __color_mapping = {'base_prompt': 'white', 'thought': 'blue', 'action': 'red', 'observation': 'yellow', 'finish': 'green'}\n",
    "  \n",
    "  def __init__(self, data: str, type_of_node: str, depth: int = 0) -> None:\n",
    "    assert type_of_node in self.__type_of_nodes, f\"Type of node must be one of {self.__type_of_nodes}\"\n",
    "    self.type_of_node = type_of_node   \n",
    "    self.depth = depth\n",
    "    if self.depth == 0:\n",
    "      assert self.type_of_node == 'base_prompt', f\"Type of node must be 'base_prompt' for depth 0\"\n",
    "    self.next = None\n",
    "    self.data = data\n",
    "    self.is_leaf = False\n",
    "    if(self.type_of_node == 'finish'):\n",
    "      self.is_leaf = True\n",
    "      \n",
    "  def add(self, data: str, type_of_node: str) -> None:\n",
    "    if self.is_leaf:\n",
    "      raise Exception(\"Cannot add to a leaf node\")\n",
    "    if self.next is None:\n",
    "      self.next = ChainTrace(data, type_of_node, self.depth+1)\n",
    "    else:\n",
    "      self.next.add(data, type_of_node)\n",
    "      \n",
    "  def compose(self) -> str:\n",
    "    if self.is_leaf or self.next is None:\n",
    "      return self.data\n",
    "    else:\n",
    "      return self.data + self.next.compose()\n",
    "      \n",
    "  def get_max_depth(self) -> int:\n",
    "    if self.is_leaf == True or self.next is None:\n",
    "      return self.depth\n",
    "    else:\n",
    "      return self.next.get_max_depth()\n",
    "    \n",
    "  def get_deepest_node(self):\n",
    "    if self.next is None:\n",
    "      return self\n",
    "    else:\n",
    "      return self.next.get_deepest_node()\n",
    "    \n",
    "  def __str__(self) -> str:\n",
    "    trace = ''\n",
    "    trace += colored(self.data, self.__color_mapping[self.type_of_node])\n",
    "    while self.next is not None:\n",
    "      self = self.next\n",
    "      trace += colored(self.data, self.__color_mapping[self.type_of_node])\n",
    "    return trace\n",
    "\n",
    "  def __len__(self) -> int:\n",
    "    return len(self.compose())\n",
    "   \n",
    "@dataclass\n",
    "class Action:\n",
    "  name: str\n",
    "  func: Callable[[str], str]\n",
    "  prefix: str\n",
    "\n",
    "  def check(self, input: str) -> bool:\n",
    "    \"\"\"\n",
    "      Check if the action is appropriate for the given action by comparing prefix attribute with input in form: 'Action: Prefix[When was Aristotle born?]'\n",
    "    \"\"\"\n",
    "    regex_rule = f\"{self.prefix}\\[.*?\\]\"\n",
    "    if len(re.findall(regex_rule, input)) != 0 and len(re.findall(regex_rule, input)) == 1:\n",
    "      return True\n",
    "    return False\n",
    "\n",
    "  def extract_query(self, input: str) -> str:\n",
    "    \"\"\"\n",
    "      This method checks if the input has a matching command for the action and then extracts the inside of the command\n",
    "      For an input with a command e.g 'Action 1: Search[When was Aristotle born?]' should return: When was Aristotle born?\n",
    "    \"\"\"\n",
    "    rule_for_cleaning = f\"{self.prefix}\\[.*?\\]\"\n",
    "    cleaned_input = re.findall(rule_for_cleaning, input)\n",
    "    assert len(cleaned_input) != 0, f\"Did not find a matching command in the input: {input}\"\n",
    "    assert len(cleaned_input) == 1, f\"Input has multiple commands input: {input}\"\n",
    "    cleaned_input = cleaned_input[0].replace(self.prefix+'[', '').replace(']', '')\n",
    "    return cleaned_input\n",
    "    \n",
    "  def __call__(self, input: str):\n",
    "    \"\"\"\n",
    "      The call should receive input in command form, e.g.: 'Search[When was Aristotle born?]'\n",
    "      Then passes extracted content, e.g. When was Aristotle born?\n",
    "    \"\"\"\n",
    "    query = self.extract_query(input)\n",
    "    return self.func(query)\n",
    "\n",
    "class Agent:\n",
    "  \"\"\" An agent implementing \"ReAct: Synergizing Reasoning and Acting in Language Models, Yao 2022\"\n",
    "  \"\"\"\n",
    "  def __init__(self, prompt: str, actions: List[Action]) -> None:\n",
    "    self.llm = Bloom(API_KEY)\n",
    "    self.base_prompt = prompt   \n",
    "    self.actions = actions \n",
    "    self.sequence_stopper = Action(name=\"Sequence stopper\", func=lambda x: x, prefix=\"Finish\")\n",
    "    self.trace = None\n",
    "    \n",
    "  def run(self, user_question: str):\n",
    "    input = self.base_prompt.format(user_question=user_question)\n",
    "    self.trace = ChainTrace(input, 'base_prompt') # initialize the trace with the base prompt\n",
    "    output = self.llm.query(input)[0][\"generated_text\"]\n",
    "    intermediate_answer = output[len(self.trace):]\n",
    "    print(intermediate_answer)\n",
    "    \n",
    "    max_iterations = 20\n",
    "    flag = True\n",
    "    while flag and max_iterations > 0:\n",
    "      max_iterations -= 1\n",
    "      for i, generated_line in enumerate(intermediate_answer.split('\\n')):\n",
    "        # TODO: add \"Thought\" at the beginning of the line for the first thought\n",
    "        if generated_line.startswith('Thought') and generated_line.endswith('\\n'):\n",
    "          self.trace.add(generated_line+'\\n', 'thought') \n",
    "               \n",
    "        elif generated_line.startswith('Thought') and not generated_line.endswith('\\n'):\n",
    "          completed_thought = self.llm.query(self.trace.compose()+generated_line)[0][\"generated_text\"]\n",
    "          completed_thought = completed_thought[len(self.trace):].split('\\n')[0]\n",
    "          self.trace.add(completed_thought+'\\n', 'thought')\n",
    "          \n",
    "        elif generated_line.startswith('Action') and not generated_line.endswith('\\n'):\n",
    "          completed_action = self.llm.query(self.trace.compose()+generated_line)[0][\"generated_text\"]\n",
    "          completed_action = completed_action[len(self.trace):].split('\\n')[0]\n",
    "          self.trace.add(completed_action+'\\n', 'action')\n",
    "          if self.sequence_stopper.check(completed_action):\n",
    "            final_answer = self.sequence_stopper(completed_action)\n",
    "            self.trace.add(final_answer+'\\n', 'finish')\n",
    "            flag = False\n",
    "            break\n",
    "          for action in self.actions:\n",
    "            if action.check(completed_action):\n",
    "              retrieved_context = action(completed_action)\n",
    "              retrieved_context = f\"Observation: {retrieved_context}\"\n",
    "              self.trace.add(retrieved_context+'\\n', 'observation')\n",
    "              \n",
    "        elif generated_line.startswith('Action') and generated_line.endswith('\\n'):\n",
    "          self.trace.add(generated_line+'\\n', 'action')\n",
    "          if self.sequence_stopper.check(completed_action):\n",
    "            final_answer = self.sequence_stopper(completed_action)\n",
    "            self.trace.add(final_answer+'\\n', 'finish')\n",
    "            flag = False\n",
    "            break\n",
    "          for action in self.actions:\n",
    "            if action.check(generated_line):\n",
    "              retrieved_context = action(generated_line)\n",
    "              self.trace.add(retrieved_context+'\\n', 'observation')\n",
    "\n",
    "      intermediate_answer = self.llm.query({\"inputs\":self.trace.compose(), \"return_full_text\": False})[0][\"generated_text\"] # should add intermidiate answer to the trace?\n",
    "      intermediate_answer = intermediate_answer[len(self.trace):]\n",
    "      print(colored(intermediate_answer, 'black'))\n",
    "    print(self.trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "react_prompt = \"\"\"Question: What profession does Nicholas Ray and Elia Kazan have in common?\n",
    "Thought 1: I need to search Nicholas Ray and Elia Kazan, find their professions, then find the profession they have in common.\n",
    "Action 1: Search[Nicholas Ray]\n",
    "Observation: Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16, 1979) was an American film director, screenwriter, and actor best known for the 1955 film Rebel Without a Cause.\n",
    "Thought 2: Professions of Nicholas Ray are director, screenwriter, and actor. I need to search Elia Kazan next and find his professions.\n",
    "Action 2: Search[Elia Kazan]\n",
    "Observation: Elia Kazan was an American film and theatre director, producer, screenwriter and actor.\n",
    "Thought 3: Professions of Elia Kazan are director, producer, screenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in common is director, screenwriter, and actor.\n",
    "Action 3: Finish[director, screenwriter, actor]\n",
    "---\n",
    "Question: Which magazine was started first Arthur’s Magazine or First for Women?\n",
    "Thought 1: I need to search Arthur’s Magazine and First for Women, and find which was started first.\n",
    "Action 1: Search[Arthur’s Magazine]\n",
    "Observation: Arthur’s Magazine (1844-1846) was an American literary periodical published in Philadelphia in the 19th century.\n",
    "Thought 2: Arthur’s Magazine was started in 1844. I need to search First for Women next.\n",
    "Action 2: Search[First for Women]\n",
    "Observation: First for Women is a woman’s magazine published by Bauer Media Group in the USA.[1] The magazine was started in 1989.\n",
    "Thought 3: First for Women was started in 1989. 1844 < 1989, so Arthur’s Magazine was started first.\n",
    "Action 3: Finish[Arthur’s Magazine]\n",
    "---\n",
    "Question: Were Pavel Urysohn and Leonid Levin known for the same type of work?\n",
    "Thought 1: I need to search Pavel Urysohn and Leonid Levin, find their types of work, then find if they are the same.\n",
    "Action 1: Search[Pavel Urysohn]\n",
    "Observation: Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet mathematician who is best known for his contributions in dimension theory.\n",
    "Thought 2: Pavel Urysohn is a mathematician. I need to search Leonid Levin next and find its type of work.\n",
    "Action 2: Search[Leonid Levin]\n",
    "Observation: Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist.\n",
    "Thought 3: Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn and Leonid Levin have the same type of work.\n",
    "Action 3: Finish[yes]\n",
    "---\n",
    "Question: {user_question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = Action(name=\"Wikipedia\", func=search_wikipedia, prefix=\"Search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(prompt=react_prompt, actions=[wiki])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1: I need to search Finnish rock groups, Adam Clayton Powell and The Saimaa\n",
      "\u001b[30mAction 1: Search[Finnish rock groups]\n",
      "Observation: Finnish rock groups is a list of\u001b[0m\n",
      "\u001b[30mThought 2: Finnish rock groups are Adam Clayton Powell and The Saimaa Gesture. I\u001b[0m\n",
      "\u001b[30mAction 2: Search[The Saimaa Gesture]\n",
      "Observation: The Saimaa Gesture\u001b[0m\n",
      "\u001b[30mThought 3: The Saimaa Gesture is a documentary about Finnish rock groups. So Finnish rock\u001b[0m\n",
      "\u001b[30mAction 3: Finish[Adam Clayton Powell]\n",
      "---\n",
      "Question: Which of the following is a type\u001b[0m\n",
      "\u001b[30mThe Saimaa Gesture\n",
      "---\n",
      "Question: Which of the following is a type of work that\u001b[0m\n",
      "\u001b[37mQuestion: What profession does Nicholas Ray and Elia Kazan have in common?\n",
      "Thought 1: I need to search Nicholas Ray and Elia Kazan, find their professions, then find the profession they have in common.\n",
      "Action 1: Search[Nicholas Ray]\n",
      "Observation: Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16, 1979) was an American film director, screenwriter, and actor best known for the 1955 film Rebel Without a Cause.\n",
      "Thought 2: Professions of Nicholas Ray are director, screenwriter, and actor. I need to search Elia Kazan next and find his professions.\n",
      "Action 2: Search[Elia Kazan]\n",
      "Observation: Elia Kazan was an American film and theatre director, producer, screenwriter and actor.\n",
      "Thought 3: Professions of Elia Kazan are director, producer, screenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in common is director, screenwriter, and actor.\n",
      "Action 3: Finish[director, screenwriter, actor]\n",
      "---\n",
      "Question: Which magazine was started first Arthur’s Magazine or First for Women?\n",
      "Thought 1: I need to search Arthur’s Magazine and First for Women, and find which was started first.\n",
      "Action 1: Search[Arthur’s Magazine]\n",
      "Observation: Arthur’s Magazine (1844-1846) was an American literary periodical published in Philadelphia in the 19th century.\n",
      "Thought 2: Arthur’s Magazine was started in 1844. I need to search First for Women next.\n",
      "Action 2: Search[First for Women]\n",
      "Observation: First for Women is a woman’s magazine published by Bauer Media Group in the USA.[1] The magazine was started in 1989.\n",
      "Thought 3: First for Women was started in 1989. 1844 < 1989, so Arthur’s Magazine was started first.\n",
      "Action 3: Finish[Arthur’s Magazine]\n",
      "---\n",
      "Question: Were Pavel Urysohn and Leonid Levin known for the same type of work?\n",
      "Thought 1: I need to search Pavel Urysohn and Leonid Levin, find their types of work, then find if they are the same.\n",
      "Action 1: Search[Pavel Urysohn]\n",
      "Observation: Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet mathematician who is best known for his contributions in dimension theory.\n",
      "Thought 2: Pavel Urysohn is a mathematician. I need to search Leonid Levin next and find its type of work.\n",
      "Action 2: Search[Leonid Levin]\n",
      "Observation: Leonid Anatolievich Levin is a Soviet-American mathematician and computer scientist.\n",
      "Thought 3: Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn and Leonid Levin have the same type of work.\n",
      "Action 3: Finish[yes]\n",
      "---\n",
      "Question: Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?\n",
      "\u001b[0m\u001b[34mThought 1: I need to search Finnish rock groups, Adam Clayton Powell and The Saimaa Gesture, and find which is about Finnish rock groups.\n",
      "\u001b[0m\u001b[31mAction 1: Search[Finnish rock groups]\n",
      "\u001b[0m\u001b[33mObservation: Finnish rock (Finnish: suomirock or suomirokki—also known as Finnsrock, Finnrock or Finrock) refers to rock music made in Finland. The initial rock and roll boom of the 1950s was preceded by a long tradition of popular culture.\n",
      "\u001b[0m\u001b[34mThought 2: Finnish rock groups are Adam Clayton Powell and The Saimaa Gesture. I need to search The Saimaa Gesture next.\n",
      "\u001b[0m\u001b[31mAction 2: Search[The Saimaa Gesture]\n",
      "\u001b[0m\u001b[33mObservation: The Saimaa Gesture (Finnish: Saimaa-ilmiö) is a 1981 film by Finnish directors Aki and Mika Kaurismäki. It is a documentary about three Finnish rock groups aboard the steamboat SS Heinävesi on their tour around Lake Saimaa.\n",
      "\u001b[0m\u001b[34mThought 3: The Saimaa Gesture is a documentary about Finnish rock groups. So Finnish rock groups are Adam Clayton Powell and The Saimaa Gesture.\n",
      "\u001b[0m\u001b[31mAction 3: Finish[Adam Clayton Powell]\n",
      "\u001b[0m\u001b[32mAdam Clayton Powell\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agent.run(\"Which documentary is about Finnish rock groups, Adam Clayton Powell or The Saimaa Gesture?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "PageError",
     "evalue": "Page id \"unit 231\" does not match any pages. Try another id!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPageError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8212/2964655962.py\u001b[0m in \u001b[0;36msearch_wikipedia\u001b[1;34m(query, num_sentences, verbose)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mwikipedia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mwikipedia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPageError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\wikipedia\\util.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(title, sentences, chars, auto_suggest, redirect)\u001b[0m\n\u001b[0;32m    230\u001b[0m   \u001b[1;31m# also, use page's error checking to raise DisambiguationError if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m   \u001b[0mpage_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauto_suggest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mauto_suggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m   \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36mpage\u001b[1;34m(title, pageid, auto_suggest, redirect, preload)\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mpageid\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, title, pageid, redirect, preload, original_title)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self, redirect, preload)\u001b[0m\n\u001b[0;32m    344\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'title'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPageError\u001b[0m: Page id \"nicholas ray\" does not match any pages. Try another id!",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPageError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8212/3779072701.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msearch_wikipedia\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Nicholar Ray\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8212/2964655962.py\u001b[0m in \u001b[0;36msearch_wikipedia\u001b[1;34m(query, num_sentences, verbose)\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossible_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{i+1}. {topic}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mwikipedia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossible_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mwikipedia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDisambiguationError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m# print(e.options)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\wikipedia\\util.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(title, sentences, chars, auto_suggest, redirect)\u001b[0m\n\u001b[0;32m    229\u001b[0m   \u001b[1;31m# use auto_suggest and redirect to get the correct article\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m   \u001b[1;31m# also, use page's error checking to raise DisambiguationError if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m   \u001b[0mpage_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauto_suggest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mauto_suggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m   \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m   \u001b[0mpageid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36mpage\u001b[1;34m(title, pageid, auto_suggest, redirect, preload)\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;31m# if there is no suggestion or search results, the page doesn't exist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mpageid\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, title, pageid, redirect, preload, original_title)\u001b[0m\n\u001b[0;32m    297\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Either a title or a pageid must be specified\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self, redirect, preload)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'missing'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'title'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPageError\u001b[0m: Page id \"unit 231\" does not match any pages. Try another id!"
     ]
    }
   ],
   "source": [
    "search_wikipedia(\"Nicholar Ray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yellow is the color between green and orange on the spectrum of light. It is evoked by light with a dominant wavelength of roughly 575–585 nm.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_wikipedia(\"color yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1: I need to search color yellow, find its definition, then find if it is the\n",
      "\u001b[30mAction 1: Search[color yellow]\n",
      "Observation: Color yellow is a color in the visible spectrum.\u001b[0m\n",
      "\u001b[30mThought 2: Color yellow is evoked by light with a dominant wavelength of roughly 575–585 nm\u001b[0m\n",
      "\u001b[30mAction 2: Search[the same]\n",
      "Observation: The same is a word used to refer to something\u001b[0m\n"
     ]
    },
    {
     "ename": "PageError",
     "evalue": "Page id \"samness\" does not match any pages. Try another id!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDisambiguationError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8832/2964655962.py\u001b[0m in \u001b[0;36msearch_wikipedia\u001b[1;34m(query, num_sentences, verbose)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mwikipedia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mwikipedia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPageError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\wikipedia\\util.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(title, sentences, chars, auto_suggest, redirect)\u001b[0m\n\u001b[0;32m    230\u001b[0m   \u001b[1;31m# also, use page's error checking to raise DisambiguationError if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m   \u001b[0mpage_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauto_suggest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mauto_suggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m   \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36mpage\u001b[1;34m(title, pageid, auto_suggest, redirect, preload)\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mpageid\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, title, pageid, redirect, preload, original_title)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self, redirect, preload)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 393\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mDisambiguationError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'title'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmay_refer_to\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDisambiguationError\u001b[0m: \"Same\" may refer to: \nSameness\nSame (Homer)\nSame (polis)\nSame, East Timor\nSamé\nSame, Tanzania\nSame District\nSAME Deutz-Fahr\nSAME (tractors)\nS-adenosyl methionine\nSociety of American Military Engineers\nSpecific Area Message Encoding\nGovernor Francisco Gabrielli International Airport\nFinal Straw\nAge Of\nThe Same\nSyndrome of apparent mineralocorticoid excess\nSistema de Atención Médica de Emergencia\nSameGame\nSam (disambiguation)\nSami (disambiguation)\nSimilarity (disambiguation)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPageError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8832/792803811.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"What is color yellow?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8832/2185553925.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, user_question)\u001b[0m\n\u001b[0;32m    129\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompleted_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m               \u001b[0mretrieved_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompleted_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m               \u001b[0mretrieved_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"Observation: {retrieved_context}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretrieved_context\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'observation'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8832/2185553925.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \"\"\"\n\u001b[0;32m     86\u001b[0m     \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8832/2964655962.py\u001b[0m in \u001b[0;36msearch_wikipedia\u001b[1;34m(query, num_sentences, verbose)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{i+1}. {topic}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mwikipedia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\wikipedia\\util.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(title, sentences, chars, auto_suggest, redirect)\u001b[0m\n\u001b[0;32m    229\u001b[0m   \u001b[1;31m# use auto_suggest and redirect to get the correct article\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m   \u001b[1;31m# also, use page's error checking to raise DisambiguationError if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m   \u001b[0mpage_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauto_suggest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mauto_suggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m   \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m   \u001b[0mpageid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36mpage\u001b[1;34m(title, pageid, auto_suggest, redirect, preload)\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;31m# if there is no suggestion or search results, the page doesn't exist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mpageid\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, title, pageid, redirect, preload, original_title)\u001b[0m\n\u001b[0;32m    297\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Either a title or a pageid must be specified\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\wikipedia\\wikipedia.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self, redirect, preload)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'missing'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'title'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpageid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPageError\u001b[0m: Page id \"samness\" does not match any pages. Try another id!"
     ]
    }
   ],
   "source": [
    "agent.run(\"What is color yellow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New Zealand',\n",
       " 'New York City',\n",
       " 'New York (state)',\n",
       " 'The New York Times',\n",
       " 'List of United States representatives from New York',\n",
       " 'New York',\n",
       " 'New York Yankees',\n",
       " 'New Deal',\n",
       " 'New York Knicks',\n",
       " 'New Orleans']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.search(\"[New]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_wikipedia(query: str, num_sentences: int = 2, verbose: bool = False) -> str:\n",
    "    try:\n",
    "        print(f\"=== {query} ===\")\n",
    "        return wikipedia.summary(query, sentences=num_sentences)\n",
    "    except wikipedia.exceptions.PageError as e:\n",
    "        possible_results = wikipedia.search(f\"[{query}]\")\n",
    "        return f\"Could not find {query}. Similar: {possible_results[:5]}.\"\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        possible_results = wikipedia.search(f\"[{query}]\")\n",
    "        return f\"Could not find {query}. Similar: {possible_results[:5]}.\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== the same ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Could not find the same. Similar: ['Same', 'Same-sex marriage', 'Same Same But Different', 'Not the Same', 'Same Same'].\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_wikipedia(\"the same\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import time\n",
    "# import gym\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import wikipedia\n",
    "\n",
    "def clean_str(p):\n",
    "  return p.encode().decode(\"unicode-escape\").encode(\"latin1\").decode(\"utf-8\")\n",
    "\n",
    "\n",
    "class textSpace():\n",
    "  def contains(self, x) -> bool:\n",
    "    \"\"\"Return boolean specifying if x is a valid member of this space.\"\"\"\n",
    "    return isinstance(x, str)\n",
    "\n",
    "\n",
    "class WikiEnv():\n",
    "\n",
    "  def __init__(self):\n",
    "    \"\"\"\n",
    "      Initialize the environment.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.page = None  # current Wikipedia page\n",
    "    self.obs = None  # current observation\n",
    "    self.lookup_keyword = None  # current lookup keyword\n",
    "    self.lookup_list = None  # list of paragraphs containing current lookup keyword\n",
    "    self.lookup_cnt = None  # current lookup index\n",
    "    self.steps = 0  # current number of steps\n",
    "    self.answer = None  # current answer from the agent\n",
    "    self.observation_space = self.action_space = textSpace()\n",
    "    self.search_time = 0\n",
    "    self.num_searches = 0\n",
    "    \n",
    "  def _get_obs(self):\n",
    "    return self.obs\n",
    "\n",
    "  def _get_info(self):\n",
    "    return {\"steps\": self.steps, \"answer\": self.answer}\n",
    "\n",
    "  def reset(self, seed=None, return_info=False, options=None):\n",
    "    # We need the following line to seed self.np_random\n",
    "    # super().reset(seed=seed)\n",
    "    self.obs = (\"Interact with Wikipedia using search[], lookup[], and \"\n",
    "                \"finish[].\\n\")\n",
    "    self.page = None\n",
    "    self.lookup_keyword = None\n",
    "    self.lookup_list = None\n",
    "    self.lookup_cnt = None\n",
    "    self.steps = 0\n",
    "    self.answer = None\n",
    "    observation = self._get_obs()\n",
    "    info = self._get_info()\n",
    "    return (observation, info) if return_info else observation\n",
    "\n",
    "  def construct_lookup_list(self, keyword):\n",
    "    # find all paragraphs\n",
    "    if self.page is None:\n",
    "      return []\n",
    "    paragraphs = self.page.split(\"\\n\")\n",
    "    paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "\n",
    "    # find all sentence\n",
    "    sentences = []\n",
    "    for p in paragraphs:\n",
    "      sentences += p.split('. ')\n",
    "    sentences = [s.strip() + '.' for s in sentences if s.strip()]\n",
    "\n",
    "    parts = sentences\n",
    "    parts = [p for p in parts if keyword.lower() in p.lower()]\n",
    "    return parts\n",
    "\n",
    "  @staticmethod\n",
    "  def get_page_obs(page):\n",
    "    # find all paragraphs\n",
    "    paragraphs = page.split(\"\\n\")\n",
    "    paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "\n",
    "    # find all sentence\n",
    "    sentences = []\n",
    "    for p in paragraphs:\n",
    "      sentences += p.split('. ')\n",
    "    sentences = [s.strip() + '.' for s in sentences if s.strip()]\n",
    "    return ' '.join(sentences[:5])\n",
    "\n",
    "    # ps = page.split(\"\\n\")\n",
    "    # ret = ps[0]\n",
    "    # for i in range(1, len(ps)):\n",
    "    #   if len((ret + ps[i]).split(\" \")) <= 50:\n",
    "    #     ret += ps[i]\n",
    "    #   else:\n",
    "    #     break\n",
    "    # return ret\n",
    "\n",
    "  def search_step(self, entity):\n",
    "    entity_ = entity.replace(\" \", \"+\")\n",
    "    search_url = f\"https://en.wikipedia.org/w/index.php?search={entity_}\"\n",
    "    old_time = time.time()\n",
    "    response_text = requests.get(search_url).text\n",
    "    self.search_time += time.time() - old_time\n",
    "    self.num_searches += 1\n",
    "    soup = BeautifulSoup(response_text, features=\"html.parser\")\n",
    "    result_divs = soup.find_all(\"div\", {\"class\": \"mw-search-result-heading\"})\n",
    "    if result_divs:  # mismatch\n",
    "      self.result_titles = [clean_str(div.get_text().strip()) for div in result_divs]\n",
    "      self.obs = f\"Could not find {entity}. Similar: {self.result_titles[:5]}.\"\n",
    "    else:\n",
    "      page = [p.get_text().strip() for p in soup.find_all(\"p\") + soup.find_all(\"ul\")]\n",
    "      if any(\"may refer to:\" in p for p in page):\n",
    "        self.search_step(\"[\" + entity + \"]\")\n",
    "      else:\n",
    "        self.page = \"\"\n",
    "        for p in page:\n",
    "          if len(p.split(\" \")) > 2:\n",
    "            self.page += clean_str(p)\n",
    "            if not p.endswith(\"\\n\"):\n",
    "              self.page += \"\\n\"\n",
    "        self.obs = self.get_page_obs(self.page)\n",
    "        self.lookup_keyword = self.lookup_list = self.lookup_cnt = None\n",
    "  \n",
    "  def step(self, action):\n",
    "    reward = 0\n",
    "    done = False\n",
    "    action = action.strip()\n",
    "    if self.answer is not None:  # already finished\n",
    "      done = True\n",
    "      return self.obs, reward, done, self._get_info()\n",
    "    \n",
    "    if action.startswith(\"search[\") and action.endswith(\"]\"):\n",
    "      entity = action[len(\"search[\"):-1]\n",
    "      # entity_ = entity.replace(\" \", \"_\")\n",
    "      # search_url = f\"https://en.wikipedia.org/wiki/{entity_}\"\n",
    "      self.search_step(entity)\n",
    "    elif action.startswith(\"lookup[\") and action.endswith(\"]\"):\n",
    "      keyword = action[len(\"lookup[\"):-1]\n",
    "      if self.lookup_keyword != keyword:  # reset lookup\n",
    "        self.lookup_keyword = keyword\n",
    "        self.lookup_list = self.construct_lookup_list(keyword)\n",
    "        self.lookup_cnt = 0\n",
    "      if self.lookup_cnt >= len(self.lookup_list):\n",
    "        self.obs = \"No more results.\\n\"\n",
    "      else:\n",
    "        self.obs = f\"(Result {self.lookup_cnt + 1} / {len(self.lookup_list)}) \" + self.lookup_list[self.lookup_cnt]\n",
    "        self.lookup_cnt += 1\n",
    "    elif action.startswith(\"finish[\") and action.endswith(\"]\"):\n",
    "      answer = action[len(\"finish[\"):-1]\n",
    "      self.answer = answer\n",
    "      done = True\n",
    "      self.obs = f\"Episode finished, reward = {reward}\\n\"\n",
    "    elif action.startswith(\"think[\") and action.endswith(\"]\"):\n",
    "      self.obs = \"Nice thought.\"\n",
    "    else:\n",
    "      self.obs = \"Invalid action: {}\".format(action)\n",
    "\n",
    "    self.steps += 1\n",
    "\n",
    "    return self.obs, reward, done, self._get_info()\n",
    "  \n",
    "  def get_time_info(self):\n",
    "    speed = self.search_time / self.num_searches if self.num_searches else 0\n",
    "    return {\n",
    "        \"call_speed\": speed,\n",
    "        \"call_time\": self.search_time,\n",
    "        \"num_calls\": self.num_searches,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = WikiEnv()\n",
    "wiki.search_step(\"New\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Could not find [New]. Similar: ['New Zealand', 'New York City', 'New York (state)', 'The New York Times', 'List of United States representatives from New York'].\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- semantic search as action \"Search\"\n",
    "- obsługa błędów dla wikipedia API\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inzynierka",
   "language": "python",
   "name": "inzynierka"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
